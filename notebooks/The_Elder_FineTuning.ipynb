{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßô The Elder - Fine-Tuning\n",
    "\n",
    "**Base Model:** `Ishymoto/The_Elder`  \n",
    "**Output:** `Ishymoto/The_Elder-FineTuned`  \n",
    "**Dataset:** 90 wisdom examples (Bushido + Stoicism + Native American + Socratic Method)\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Instructions:\n",
    "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n",
    "2. **Add Secrets**: Click üîë icon, add `HF_TOKEN` with your Hugging Face token\n",
    "3. **Run All Cells** in order\n",
    "4. **After Cell 3**: Click \"RESTART RUNTIME\" button\n",
    "5. **After restart**: Continue from Cell 4\n",
    "\n",
    "**‚è±Ô∏è Total time:** ~1 hour\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 1: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è NO GPU! Go to: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
    "    raise SystemExit(\"GPU required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 2: Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "    os.environ['HUGGING_FACE_HUB_TOKEN'] = HF_TOKEN\n",
    "    print(\"‚úÖ HF_TOKEN loaded\")\n",
    "except:\n",
    "    print(\"‚ùå HF_TOKEN not found! Add it in Secrets (üîë icon)\")\n",
    "    raise\n",
    "\n",
    "# Configuration\n",
    "HF_USERNAME = \"Ishymoto\"\n",
    "GITHUB_USERNAME = \"Ishabdullah\"\n",
    "REPO_NAME = \"the-elder-llm\"\n",
    "BASE_MODEL = \"Ishymoto/The_Elder\"\n",
    "OUTPUT_MODEL = \"The_Elder-FineTuned\"\n",
    "\n",
    "print(f\"‚úÖ Base model: {BASE_MODEL}\")\n",
    "print(f\"‚úÖ Output model: {HF_USERNAME}/{OUTPUT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 3: Install Packages\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT:** After this cell completes, click \"RESTART RUNTIME\" button at the top!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U \\\n",
    "    transformers \\\n",
    "    datasets \\\n",
    "    accelerate \\\n",
    "    peft \\\n",
    "    trl \\\n",
    "    bitsandbytes \\\n",
    "    huggingface_hub \\\n",
    "    sentencepiece\n",
    "\n",
    "print(\"\\n‚úÖ Packages installed!\")\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT: Click 'RESTART RUNTIME' button above, then continue from CELL 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 4: Verify Install (Run AFTER Restart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import after restart\n",
    "import os\n",
    "import torch\n",
    "from google.colab import userdata\n",
    "\n",
    "# Re-load secrets\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "os.environ['HUGGING_FACE_HUB_TOKEN'] = HF_TOKEN\n",
    "\n",
    "# Re-set configuration\n",
    "HF_USERNAME = \"Ishymoto\"\n",
    "GITHUB_USERNAME = \"Ishabdullah\"\n",
    "REPO_NAME = \"the-elder-llm\"\n",
    "BASE_MODEL = \"Ishymoto/The_Elder\"\n",
    "OUTPUT_MODEL = \"The_Elder-FineTuned\"\n",
    "\n",
    "# Verify imports\n",
    "import transformers\n",
    "import datasets\n",
    "import peft\n",
    "import trl\n",
    "import bitsandbytes\n",
    "\n",
    "print(\"‚úÖ All packages ready!\")\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"torch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 5: Clone Repository & Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf the-elder-llm\n",
    "\n",
    "repo_url = f\"https://github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "!git clone {repo_url}\n",
    "%cd the-elder-llm\n",
    "\n",
    "import json\n",
    "dataset_path = \"data/the_elder_complete_dataset.jsonl\"\n",
    "\n",
    "with open(dataset_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"‚úÖ Dataset: {len(lines)} examples\")\n",
    "\n",
    "# Show sample\n",
    "sample = json.loads(lines[0])\n",
    "if 'instruction' in sample:\n",
    "    print(f\"\\nSample instruction: {sample['instruction'][:80]}...\")\n",
    "elif 'prompt' in sample:\n",
    "    print(f\"\\nSample prompt: {sample['prompt'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 6: Load Base Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "print(f\"Loading base model: {BASE_MODEL}\")\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, token=HF_TOKEN, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 7: Prepare & Format Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('json', data_files='data/the_elder_complete_dataset.jsonl', split='train')\n",
    "\n",
    "# Load system prompt\n",
    "with open('configs/the_elder_system_prompt.txt', 'r') as f:\n",
    "    system_prompt = f.read().strip()\n",
    "\n",
    "def format_instruction(sample):\n",
    "    # Handle both formats: instruction/output and prompt/completion\n",
    "    if 'instruction' in sample:\n",
    "        user_message = sample['instruction']\n",
    "        if sample.get('input', ''):\n",
    "            user_message = f\"{user_message}\\n{sample['input']}\"\n",
    "        output = sample['output']\n",
    "    elif 'prompt' in sample:\n",
    "        user_message = sample['prompt']\n",
    "        output = sample['completion']\n",
    "    else:\n",
    "        raise ValueError(\"Dataset must have either 'instruction'/'output' or 'prompt'/'completion' fields\")\n",
    "    \n",
    "    prompt = f\"\"\"<|system|>\n",
    "{system_prompt}</s>\n",
    "<|user|>\n",
    "{user_message}</s>\n",
    "<|assistant|>\n",
    "{output}</s>\"\"\"\n",
    "    \n",
    "    return {\"text\": prompt}\n",
    "\n",
    "formatted_dataset = dataset.map(format_instruction, remove_columns=dataset.column_names)\n",
    "print(f\"‚úÖ Dataset formatted: {len(formatted_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 8: Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"\\n‚úÖ LoRA configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 9: Fine-Tune! (~30-45 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./the-elder-finetuned-output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    max_grad_norm=0.3,\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    packing=False,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ FINE-TUNING THE ELDER\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset: {len(formatted_dataset)} examples\")\n",
    "print(f\"Epochs: 3\")\n",
    "print(f\"Effective batch size: 16\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ FINE-TUNING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 10: Save & Merge LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "print(\"Saving LoRA adapter...\")\n",
    "trainer.model.save_pretrained(\"./the-elder-lora-finetuned\")\n",
    "tokenizer.save_pretrained(\"./the-elder-lora-finetuned\")\n",
    "\n",
    "print(\"Merging LoRA with base model...\")\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "merged_model = PeftModel.from_pretrained(base_model_reload, \"./the-elder-lora-finetuned\")\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "merged_model.save_pretrained(\"./the-elder-merged-finetuned\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"./the-elder-merged-finetuned\")\n",
    "\n",
    "print(\"‚úÖ Model merged and saved locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 11: Test The Fine-Tuned Elder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "test_questions = [\n",
    "    \"What is true strength?\",\n",
    "    \"Should I trust someone who has wronged me before?\",\n",
    "    \"How do I know if I'm making the right decision?\",\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üßô TESTING THE FINE-TUNED ELDER\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for q in test_questions:\n",
    "    prompt = f\"<|system|>\\n{system_prompt}</s>\\n<|user|>\\n{q}</s>\\n<|assistant|>\\n\"\n",
    "    response = generator(prompt)[0]['generated_text']\n",
    "    answer = response.split(\"<|assistant|>\")[-1].split(\"</s>\")[0].strip()\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {answer}\\n\")\n",
    "    print(\"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 12: Push to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "repo_id = f\"{HF_USERNAME}/{OUTPUT_MODEL}\"\n",
    "\n",
    "print(f\"Pushing to {repo_id}...\")\n",
    "merged_model.push_to_hub(repo_id, token=HF_TOKEN)\n",
    "tokenizer.push_to_hub(repo_id, token=HF_TOKEN)\n",
    "\n",
    "print(f\"\\n‚úÖ Model live at: https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 13: Create Model Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = f\"\"\"---\n",
    "license: apache-2.0\n",
    "language: [en]\n",
    "tags: [philosophy, wisdom, stoicism, bushido, native-american-wisdom, socratic-method]\n",
    "base_model: {BASE_MODEL}\n",
    "---\n",
    "\n",
    "# üßô The Elder - Fine-Tuned Wisdom Guide\n",
    "\n",
    "A philosophical AI guide fine-tuned on Bushido, Stoicism, Native American wisdom, and the Socratic Method.\n",
    "\n",
    "## About\n",
    "\n",
    "This model is a fine-tuned version of `{BASE_MODEL}`, trained on 90 additional wisdom examples that emphasize:\n",
    "\n",
    "- **Bushido**: The way of the warrior - honor, discipline, courage\n",
    "- **Stoicism**: Marcus Aurelius, Seneca, Epictetus - control, virtue, acceptance\n",
    "- **Native American Wisdom**: Connection to nature, balance, ancestral knowledge\n",
    "- **Socratic Method**: Teaching through questions rather than direct answers\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{repo_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{repo_id}\")\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "response = generator(\"What is true strength?\", max_new_tokens=150)\n",
    "print(response[0]['generated_text'])\n",
    "```\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- **Base Model**: {BASE_MODEL}\n",
    "- **Method**: LoRA fine-tuning (r=16, Œ±=32)\n",
    "- **Dataset**: 90 wisdom Q&A pairs\n",
    "- **Epochs**: 3\n",
    "- **Batch Size**: 16 (effective)\n",
    "\n",
    "## Examples\n",
    "\n",
    "**Direct Wisdom:**\n",
    "> Q: What is true strength?\n",
    "> A: True strength is not in never falling, but in rising each time you fall...\n",
    "\n",
    "**Socratic Guidance:**\n",
    "> Q: Should I trust this person who has wronged me before?\n",
    "> A: Seeker, ask yourself: What patterns have you observed? When trust was broken, was it from circumstance or character?\n",
    "\n",
    "---\n",
    "\n",
    "*\"The warrior trains not to avoid falling, but to rise each time with greater wisdom.\"*\n",
    "\n",
    "## License\n",
    "\n",
    "Apache 2.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"./the-elder-merged-finetuned/README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"./the-elder-merged-finetuned/README.md\",\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=repo_id,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model card uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 14: Convert to GGUF (Optional)\n",
    "\n",
    "‚ö†Ô∏è This may fail in Colab due to compilation issues. If it fails, use the online converter:\n",
    "https://huggingface.co/spaces/ggml-org/gguf-my-repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Installing llama.cpp...\")\n",
    "    !git clone https://github.com/ggerganov/llama.cpp 2>/dev/null || true\n",
    "    !cd llama.cpp && make clean && make\n",
    "    !pip install -q gguf\n",
    "    \n",
    "    print(\"\\nConverting to FP16 GGUF...\")\n",
    "    !python llama.cpp/convert.py ./the-elder-merged-finetuned --outtype f16 --outfile ./the-elder-finetuned-f16.gguf\n",
    "    \n",
    "    print(\"\\nQuantizing to Q4_K_M...\")\n",
    "    !./llama.cpp/quantize ./the-elder-finetuned-f16.gguf ./The_Elder_FineTuned.gguf Q4_K_M\n",
    "    \n",
    "    import os\n",
    "    gguf_size = os.path.getsize(\"./The_Elder_FineTuned.gguf\") / (1024 * 1024)\n",
    "    print(f\"\\n‚úÖ GGUF created: {gguf_size:.2f} MB\")\n",
    "    \n",
    "    # Upload GGUF\n",
    "    print(\"\\nUploading GGUF...\")\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=\"./The_Elder_FineTuned.gguf\",\n",
    "        path_in_repo=\"The_Elder_FineTuned.gguf\",\n",
    "        repo_id=repo_id,\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "    print(f\"‚úÖ GGUF uploaded!\")\n",
    "    print(f\"üì• Download: https://huggingface.co/{repo_id}/resolve/main/The_Elder_FineTuned.gguf\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è GGUF conversion failed: {e}\")\n",
    "    print(\"\\nAlternative: Use online converter\")\n",
    "    print(f\"1. Go to: https://huggingface.co/spaces/ggml-org/gguf-my-repo\")\n",
    "    print(f\"2. Enter model ID: {repo_id}\")\n",
    "    print(f\"3. Select quantization: Q4_K_M\")\n",
    "    print(f\"4. Click 'Convert'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 15: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéâ THE ELDER - FINE-TUNING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úÖ Model: https://huggingface.co/{repo_id}\")\n",
    "print(f\"‚úÖ Base: {BASE_MODEL}\")\n",
    "print(f\"‚úÖ Dataset: {len(formatted_dataset)} examples\")\n",
    "print(f\"   - Original wisdom examples\")\n",
    "print(f\"   - Socratic method teaching\")\n",
    "print(f\"   - Bushido + Stoicism + Native American wisdom\")\n",
    "print(\"\\nüì± Next Steps:\")\n",
    "print(\"  1. Test on Hugging Face: https://huggingface.co/chat\")\n",
    "print(\"  2. Download GGUF for mobile (if converted)\")\n",
    "print(\"  3. Install in SmolChat or LM Studio\")\n",
    "print(\"\\n‚ú® May The Elder guide you on your path ‚ú®\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
