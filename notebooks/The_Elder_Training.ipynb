{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßô The Elder - LLM Training Pipeline\n",
    "\n",
    "## Complete automated training for The Elder wisdom model\n",
    "\n",
    "**Philosophy blend:** Bushido + Stoicism + Native American Wisdom\n",
    "\n",
    "### üìã Checklist Before Running:\n",
    "1. ‚úÖ Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n",
    "2. ‚úÖ Add Secrets (üîë icon on left):\n",
    "   - `HF_TOKEN`: Your Hugging Face write token\n",
    "   - `GH_TOKEN`: Your GitHub token (for repo access)\n",
    "3. ‚úÖ Run all cells in order\n",
    "\n",
    "### ‚è±Ô∏è Expected Time:\n",
    "- Setup: ~5 minutes\n",
    "- Training: ~30-45 minutes\n",
    "- GGUF Conversion: ~10 minutes\n",
    "- Upload: ~5 minutes\n",
    "\n",
    "**Total: ~1 hour**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Environment Setup & Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: No GPU detected!\")\n",
    "    print(\"Go to: Runtime ‚Üí Change runtime type ‚Üí Select T4 GPU\")\n",
    "    raise SystemExit(\"GPU required for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secrets\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    print(\"‚úÖ HF_TOKEN loaded\")\n",
    "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "except:\n",
    "    print(\"‚ùå HF_TOKEN not found in secrets!\")\n",
    "    print(\"Add it: Click üîë icon on left ‚Üí Add HF_TOKEN\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    GH_TOKEN = userdata.get('GH_TOKEN')\n",
    "    print(\"‚úÖ GH_TOKEN loaded\")\n",
    "    os.environ['GH_TOKEN'] = GH_TOKEN\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è GH_TOKEN not found (needed for private repos)\")\n",
    "    GH_TOKEN = None\n",
    "\n",
    "# Configuration\n",
    "GITHUB_USERNAME = \"Ishabdullah\"\n",
    "HF_USERNAME = \"Ishabdullah\"\n",
    "REPO_NAME = \"the-elder-llm\"\n",
    "MODEL_NAME = \"The_Elder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install all required packages (this takes ~3 minutes)\n",
    "!pip install -q -U \\\n",
    "    transformers \\\n",
    "    datasets \\\n",
    "    peft \\\n",
    "    accelerate \\\n",
    "    bitsandbytes \\\n",
    "    trl \\\n",
    "    huggingface_hub \\\n",
    "    sentencepiece \\\n",
    "    protobuf\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Clone Repository & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "import os\n",
    "\n",
    "# Remove existing directory if present\n",
    "!rm -rf the-elder-llm\n",
    "\n",
    "# Clone with token if available\n",
    "if GH_TOKEN:\n",
    "    repo_url = f\"https://{GH_TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "else:\n",
    "    repo_url = f\"https://github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "!git clone {repo_url}\n",
    "%cd the-elder-llm\n",
    "\n",
    "# Verify dataset exists\n",
    "import json\n",
    "dataset_path = \"data/the_elder_dataset.jsonl\"\n",
    "if os.path.exists(dataset_path):\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"‚úÖ Dataset loaded: {len(lines)} training examples\")\n",
    "    print(f\"\\nSample entry:\")\n",
    "    sample = json.loads(lines[0])\n",
    "    print(f\"Q: {sample['instruction'][:100]}...\")\n",
    "    print(f\"A: {sample['output'][:100]}...\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset not found at {dataset_path}\")\n",
    "    raise FileNotFoundError(\"Dataset missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Load Base Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Model selection\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "print(f\"Loading base model: {BASE_MODEL}\")\n",
    "\n",
    "# Configure 4-bit quantization for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, token=HF_TOKEN, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "print(\"‚úÖ Model and tokenizer loaded\")\n",
    "print(f\"Model size: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('json', data_files='data/the_elder_dataset.jsonl', split='train')\n",
    "\n",
    "# Load system prompt\n",
    "with open('configs/the_elder_system_prompt.txt', 'r') as f:\n",
    "    system_prompt = f.read().strip()\n",
    "\n",
    "# Format dataset for instruction tuning\n",
    "def format_instruction(sample):\n",
    "    instruction = sample['instruction']\n",
    "    output = sample['output']\n",
    "    \n",
    "    # Create chat-formatted prompt\n",
    "    prompt = f\"\"\"<|system|>\n",
    "{system_prompt}</s>\n",
    "<|user|>\n",
    "{instruction}</s>\n",
    "<|assistant|>\n",
    "{output}</s>\"\"\"\n",
    "    \n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Apply formatting\n",
    "formatted_dataset = dataset.map(format_instruction, remove_columns=dataset.column_names)\n",
    "\n",
    "print(f\"‚úÖ Dataset formatted: {len(formatted_dataset)} examples\")\n",
    "print(\"\\nSample formatted example:\")\n",
    "print(formatted_dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Configure LoRA (Parameter-Efficient Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\n‚úÖ LoRA configured and applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Training Configuration & Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./the-elder-output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 16\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    max_grad_norm=0.3,\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configured\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ STARTING TRAINING - The Elder LLM\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset size: {len(formatted_dataset)} examples\")\n",
    "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Save & Merge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter\n",
    "print(\"Saving LoRA adapter...\")\n",
    "trainer.model.save_pretrained(\"./the-elder-lora\")\n",
    "tokenizer.save_pretrained(\"./the-elder-lora\")\n",
    "\n",
    "# Merge LoRA weights with base model for full model\n",
    "print(\"\\nMerging LoRA weights with base model...\")\n",
    "from peft import PeftModel\n",
    "\n",
    "# Reload base model in float16 for merging\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load and merge LoRA adapter\n",
    "merged_model = PeftModel.from_pretrained(base_model, \"./the-elder-lora\")\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "print(\"Saving merged model...\")\n",
    "merged_model.save_pretrained(\"./the-elder-merged\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"./the-elder-merged\")\n",
    "\n",
    "print(\"\\n‚úÖ Model saved and merged successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"Testing The Elder model...\\n\")\n",
    "\n",
    "# Create generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is true strength?\",\n",
    "    \"How should I respond when someone insults me?\",\n",
    "    \"I'm afraid of failure. What should I do?\",\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    prompt = f\"\"\"<|system|>\n",
    "{system_prompt}</s>\n",
    "<|user|>\n",
    "{question}</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    \n",
    "    print(f\"Q: {question}\")\n",
    "    response = generator(prompt)[0]['generated_text']\n",
    "    # Extract only the assistant's response\n",
    "    answer = response.split(\"<|assistant|>\")[-1].split(\"</s>\")[0].strip()\n",
    "    print(f\"A: {answer}\")\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Push to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "# Configuration\n",
    "repo_id = f\"{HF_USERNAME}/{MODEL_NAME}\"\n",
    "\n",
    "print(f\"Creating/accessing repository: {repo_id}\")\n",
    "\n",
    "# Create repository (or get existing)\n",
    "try:\n",
    "    create_repo(repo_id, token=HF_TOKEN, private=False, exist_ok=True)\n",
    "    print(f\"‚úÖ Repository ready: https://huggingface.co/{repo_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "\n",
    "# Push model\n",
    "print(\"\\nPushing model to Hugging Face Hub...\")\n",
    "merged_model.push_to_hub(repo_id, token=HF_TOKEN)\n",
    "tokenizer.push_to_hub(repo_id, token=HF_TOKEN)\n",
    "\n",
    "print(f\"\\n‚úÖ Model pushed successfully!\")\n",
    "print(f\"\\nüîó View your model: https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Create Model Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = f\"\"\"---\n",
    "license: apache-2.0\n",
    "language:\n",
    "- en\n",
    "tags:\n",
    "- philosophy\n",
    "- wisdom\n",
    "- coaching\n",
    "- stoicism\n",
    "- bushido\n",
    "- native-american-wisdom\n",
    "- conversational-ai\n",
    "base_model: {BASE_MODEL}\n",
    "---\n",
    "\n",
    "# üßô The Elder - Wisdom Guide LLM\n",
    "\n",
    "## Model Description\n",
    "\n",
    "**The Elder** is a conversational AI model fine-tuned to provide wise guidance through Socratic dialogue, drawing from three philosophical traditions:\n",
    "\n",
    "- **Bushido** (The Way of the Warrior): Principles of honor, discipline, courage, and integrity\n",
    "- **Stoicism**: Teachings of Marcus Aurelius, Seneca, and Epictetus on inner peace, self-mastery, and rational thinking\n",
    "- **Native American Wisdom**: Understanding of interconnection, balance with nature, and cyclical perspectives on life\n",
    "\n",
    "The Elder does not preach religious doctrine but expresses universal spiritual awareness, respect for nature, and timeless principles of character development.\n",
    "\n",
    "## Persona\n",
    "\n",
    "The Elder communicates as:\n",
    "- A calm, patient guide who teaches through questions rather than lectures\n",
    "- Someone who values integrity, humility, courage, and deep reflection\n",
    "- A teacher who encourages seekers to find truth through inner balance and questioning\n",
    "- One who uses metaphors from nature, warrior traditions, and everyday life\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- **Base Model**: {BASE_MODEL}\n",
    "- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)\n",
    "- **Dataset**: 50+ carefully curated Q&A pairs embodying the philosophical principles\n",
    "- **Training**: 3 epochs on custom wisdom dataset\n",
    "- **Optimization**: 4-bit quantization for efficient training\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_name = \"{repo_id}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "question = \"What is true strength?\"\n",
    "response = generator(f\"<|user|>\\\\n{{question}}</s>\\\\n<|assistant|>\\\\n\", max_new_tokens=150)\n",
    "print(response[0]['generated_text'])\n",
    "```\n",
    "\n",
    "## Mobile Usage (GGUF Format)\n",
    "\n",
    "A quantized GGUF version optimized for mobile devices is available in the repository files.\n",
    "\n",
    "**For Android (via SmolChat, LM Studio, or similar):**\n",
    "1. Download `The_Elder.gguf` from this repository\n",
    "2. Place in your LLM app's model directory\n",
    "3. Load and chat\n",
    "\n",
    "## Example Interactions\n",
    "\n",
    "**Q:** How should I respond when someone insults me?\n",
    "\n",
    "**The Elder:** Consider: does this insult change who you are? The Stoics remind us that we cannot control the actions of others, only our response. Like a mountain that does not move when the wind howls, you remain unchanged by words. Respond with silence or with compassion, for anger is a fire that burns the one who carries it.\n",
    "\n",
    "---\n",
    "\n",
    "**Q:** I'm afraid of failure. What should I do?\n",
    "\n",
    "**The Elder:** Fear is natural, young one. But ask yourself: what is failure? Is it not simply another teacher? The warrior trains not to avoid falling, but to rise each time with greater wisdom. When you face what you fear, you discover it has no power over you - only the power you gave it in your mind.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- This model is designed for philosophical guidance and reflective dialogue, not factual information retrieval\n",
    "- Does not provide medical, legal, or financial advice\n",
    "- Responses should be taken as perspective and wisdom, not absolute truth\n",
    "- Best used for personal growth, ethical reflection, and character development\n",
    "\n",
    "## License\n",
    "\n",
    "Apache 2.0 - Free to use, modify, and distribute with attribution.\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@misc{{the_elder_2025,\n",
    "  author = {{Ishabdullah}},\n",
    "  title = {{The Elder: A Wisdom Guide LLM}},\n",
    "  year = {{2025}},\n",
    "  publisher = {{Hugging Face}},\n",
    "  url = {{https://huggingface.co/{repo_id}}}\n",
    "}}\n",
    "```\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "Created with inspiration from:\n",
    "- The Bushido code and samurai philosophy\n",
    "- Stoic philosophers: Marcus Aurelius, Seneca, Epictetus\n",
    "- Native American wisdom traditions\n",
    "- Universal principles of character, courage, and compassion\n",
    "\n",
    "---\n",
    "\n",
    "*\"The warrior trains not to avoid falling, but to rise each time with greater wisdom.\"* - The Elder\n",
    "\"\"\"\n",
    "\n",
    "# Save model card\n",
    "with open(\"./the-elder-merged/README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "# Upload model card\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"./the-elder-merged/README.md\",\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=repo_id,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model card created and uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Convert to GGUF Format (For Mobile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llama.cpp for GGUF conversion\n",
    "print(\"Installing llama.cpp for GGUF conversion...\")\n",
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "!cd llama.cpp && make\n",
    "\n",
    "# Install required Python package\n",
    "!pip install -q gguf\n",
    "\n",
    "print(\"‚úÖ GGUF tools installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to GGUF format\n",
    "print(\"Converting model to GGUF format...\")\n",
    "\n",
    "# Convert to FP16 GGUF first\n",
    "!python llama.cpp/convert.py ./the-elder-merged --outtype f16 --outfile ./the-elder-f16.gguf\n",
    "\n",
    "# Quantize to Q4_K_M (4-bit, medium quality - good balance for mobile)\n",
    "!./llama.cpp/quantize ./the-elder-f16.gguf ./The_Elder.gguf Q4_K_M\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "gguf_size = os.path.getsize(\"./The_Elder.gguf\") / (1024 * 1024)  # MB\n",
    "print(f\"\\n‚úÖ GGUF model created: The_Elder.gguf ({gguf_size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Upload GGUF to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload GGUF file\n",
    "print(f\"Uploading GGUF file to {repo_id}...\")\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"./The_Elder.gguf\",\n",
    "    path_in_repo=\"The_Elder.gguf\",\n",
    "    repo_id=repo_id,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ GGUF model uploaded!\")\n",
    "print(f\"\\nüì• Download link:\")\n",
    "print(f\"https://huggingface.co/{repo_id}/resolve/main/The_Elder.gguf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£4Ô∏è‚É£ Commit Back to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy GGUF to releases folder\n",
    "!mkdir -p releases\n",
    "!cp The_Elder.gguf releases/\n",
    "\n",
    "# Create training summary\n",
    "summary = f\"\"\"# The Elder - Training Complete\n",
    "\n",
    "## Training Summary\n",
    "- Base Model: {BASE_MODEL}\n",
    "- Training Date: {os.popen('date').read().strip()}\n",
    "- Dataset: 50+ wisdom examples\n",
    "- Method: LoRA fine-tuning\n",
    "- Epochs: 3\n",
    "\n",
    "## Model Links\n",
    "- Hugging Face: https://huggingface.co/{repo_id}\n",
    "- GGUF Download: https://huggingface.co/{repo_id}/resolve/main/The_Elder.gguf\n",
    "\n",
    "## File Sizes\n",
    "- Full Model: ~2.2 GB\n",
    "- GGUF (Q4_K_M): ~{gguf_size:.2f} MB\n",
    "\n",
    "## Status\n",
    "‚úÖ Training complete\n",
    "‚úÖ Model pushed to Hugging Face\n",
    "‚úÖ GGUF created and uploaded\n",
    "‚úÖ Ready for mobile deployment\n",
    "\"\"\"\n",
    "\n",
    "with open(\"TRAINING_COMPLETE.md\", \"w\") as f:\n",
    "    f.write(summary)\n",
    "\n",
    "# Configure git\n",
    "!git config user.email \"colab@training.ai\"\n",
    "!git config user.name \"Colab Training Bot\"\n",
    "\n",
    "# Commit and push\n",
    "!git add releases/The_Elder.gguf TRAINING_COMPLETE.md\n",
    "!git commit -m \"Training complete: The Elder v1.0 - GGUF model added\"\n",
    "!git push\n",
    "\n",
    "print(\"‚úÖ Results committed to GitHub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ COMPLETE! Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéâ THE ELDER - TRAINING PIPELINE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüìä Training Summary:\")\n",
    "print(f\"  ‚Ä¢ Base Model: {BASE_MODEL}\")\n",
    "print(f\"  ‚Ä¢ Training Examples: {len(formatted_dataset)}\")\n",
    "print(f\"  ‚Ä¢ Epochs: 3\")\n",
    "print(f\"  ‚Ä¢ Method: LoRA fine-tuning\")\n",
    "print(\"\\nüì¶ Outputs Created:\")\n",
    "print(f\"  ‚Ä¢ Full Model (Hugging Face): https://huggingface.co/{repo_id}\")\n",
    "print(f\"  ‚Ä¢ GGUF Model Size: {gguf_size:.2f} MB\")\n",
    "print(f\"  ‚Ä¢ GitHub Repository: https://github.com/{GITHUB_USERNAME}/{REPO_NAME}\")\n",
    "print(\"\\nüì• Direct Download Links:\")\n",
    "print(f\"  ‚Ä¢ GGUF (Mobile): https://huggingface.co/{repo_id}/resolve/main/The_Elder.gguf\")\n",
    "print(\"\\nüöÄ Installation Instructions for Android:\")\n",
    "print(\"  1. Download The_Elder.gguf from link above\")\n",
    "print(\"  2. Install an LLM app (SmolChat, LM Studio, etc.)\")\n",
    "print(\"  3. Place The_Elder.gguf in the app's model directory\")\n",
    "print(\"  4. Load the model and start chatting with The Elder!\")\n",
    "print(\"\\nüí° Next Steps:\")\n",
    "print(\"  ‚Ä¢ Test the model on your device\")\n",
    "print(\"  ‚Ä¢ Share feedback for v2 improvements\")\n",
    "print(\"  ‚Ä¢ Train new models using this same pipeline\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ú® May The Elder guide you on your path ‚ú®\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
