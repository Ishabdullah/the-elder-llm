{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßô The Elder - LLM Training Pipeline (v2 - FIXED FOR COLAB)\n",
    "\n",
    "## ‚úÖ This version resolves all dependency conflicts\n",
    "\n",
    "**Philosophy blend:** Bushido + Stoicism + Native American Wisdom\n",
    "\n",
    "### üìã Checklist:\n",
    "1. ‚úÖ Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n",
    "2. ‚úÖ Add Secrets (üîë icon):\n",
    "   - `HF_TOKEN`: Your Hugging Face token  \n",
    "   - `GH_TOKEN`: Your GitHub token\n",
    "3. ‚úÖ Run cells **in order** (some require runtime restart)\n",
    "\n",
    "### ‚è±Ô∏è Time: ~1 hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è NO GPU! Go to: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
    "    raise SystemExit(\"GPU required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "    os.environ['HUGGING_FACE_HUB_TOKEN'] = HF_TOKEN\n",
    "    print(\"‚úÖ HF_TOKEN loaded\")\n",
    "except:\n",
    "    print(\"‚ùå HF_TOKEN not found! Add it in Secrets (üîë icon)\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    GH_TOKEN = userdata.get('GH_TOKEN')\n",
    "    os.environ['GH_TOKEN'] = GH_TOKEN\n",
    "    print(\"‚úÖ GH_TOKEN loaded\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è GH_TOKEN not found (optional)\")\n",
    "    GH_TOKEN = None\n",
    "\n",
    "GITHUB_USERNAME = \"Ishabdullah\"\n",
    "HF_USERNAME = \"Ishabdullah\"\n",
    "REPO_NAME = \"the-elder-llm\"\n",
    "MODEL_NAME = \"The_Elder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Install Packages (FIXED - No version conflicts!)\n",
    "\n",
    "**IMPORTANT**: After this cell runs, you'll see a **\"RESTART RUNTIME\"** button. Click it, then continue from cell 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install with compatible versions for Colab\n",
    "!pip install -q -U \\\n",
    "    transformers \\\n",
    "    datasets \\\n",
    "    accelerate \\\n",
    "    peft \\\n",
    "    trl \\\n",
    "    bitsandbytes \\\n",
    "    huggingface_hub \\\n",
    "    sentencepiece\n",
    "\n",
    "print(\"\\n‚úÖ Packages installed!\")\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT: Click 'RESTART RUNTIME' button above, then continue from cell 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Verify Installs (Run after restart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import after restart\n",
    "import os\n",
    "import torch\n",
    "from google.colab import userdata\n",
    "\n",
    "# Re-load secrets\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "os.environ['HUGGING_FACE_HUB_TOKEN'] = HF_TOKEN\n",
    "\n",
    "try:\n",
    "    GH_TOKEN = userdata.get('GH_TOKEN')\n",
    "    os.environ['GH_TOKEN'] = GH_TOKEN\n",
    "except:\n",
    "    GH_TOKEN = None\n",
    "\n",
    "GITHUB_USERNAME = \"Ishabdullah\"\n",
    "HF_USERNAME = \"Ishabdullah\"\n",
    "REPO_NAME = \"the-elder-llm\"\n",
    "MODEL_NAME = \"The_Elder\"\n",
    "\n",
    "# Verify imports\n",
    "import transformers\n",
    "import datasets\n",
    "import peft\n",
    "import trl\n",
    "import bitsandbytes\n",
    "\n",
    "print(\"‚úÖ All packages ready!\")\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"torch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf the-elder-llm\n",
    "\n",
    "if GH_TOKEN:\n",
    "    repo_url = f\"https://{GH_TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "else:\n",
    "    repo_url = f\"https://github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "!git clone {repo_url}\n",
    "%cd the-elder-llm\n",
    "\n",
    "import json\n",
    "dataset_path = \"data/the_elder_dataset.jsonl\"\n",
    "with open(dataset_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"‚úÖ Dataset: {len(lines)} examples\")\n",
    "sample = json.loads(lines[0])\n",
    "print(f\"\\nSample Q: {sample['instruction'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "print(f\"Loading: {BASE_MODEL}\")\n",
    "\n",
    "# 4-bit config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, token=HF_TOKEN, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('json', data_files='data/the_elder_dataset.jsonl', split='train')\n",
    "\n",
    "with open('configs/the_elder_system_prompt.txt', 'r') as f:\n",
    "    system_prompt = f.read().strip()\n",
    "\n",
    "def format_instruction(sample):\n",
    "    instruction = sample['instruction']\n",
    "    input_text = sample.get('input', '')\n",
    "    output = sample['output']\n",
    "    \n",
    "    user_message = f\"{instruction}\\n{input_text}\" if input_text else instruction\n",
    "    \n",
    "    prompt = f\"\"\"<|system|>\n",
    "{system_prompt}</s>\n",
    "<|user|>\n",
    "{user_message}</s>\n",
    "<|assistant|>\n",
    "{output}</s>\"\"\"\n",
    "    \n",
    "    return {\"text\": prompt}\n",
    "\n",
    "formatted_dataset = dataset.map(format_instruction, remove_columns=dataset.column_names)\n",
    "print(f\"‚úÖ Dataset formatted: {len(formatted_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"\\n‚úÖ LoRA configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Train! (~30-45 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./the-elder-output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    max_grad_norm=0.3,\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ TRAINING THE ELDER\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset: {len(formatted_dataset)} examples\")\n",
    "print(f\"Epochs: 3\")\n",
    "print(f\"Effective batch size: 16\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Save & Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "print(\"Saving LoRA...\")\n",
    "trainer.model.save_pretrained(\"./the-elder-lora\")\n",
    "tokenizer.save_pretrained(\"./the-elder-lora\")\n",
    "\n",
    "print(\"Merging...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "merged_model = PeftModel.from_pretrained(base_model, \"./the-elder-lora\")\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "merged_model.save_pretrained(\"./the-elder-merged\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"./the-elder-merged\")\n",
    "\n",
    "print(\"‚úÖ Model merged and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Test The Elder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "test_questions = [\n",
    "    \"What is true strength?\",\n",
    "    \"How should I respond when someone insults me?\",\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    prompt = f\"<|system|>\\n{system_prompt}</s>\\n<|user|>\\n{q}</s>\\n<|assistant|>\\n\"\n",
    "    response = generator(prompt)[0]['generated_text']\n",
    "    answer = response.split(\"<|assistant|>\")[-1].split(\"</s>\")[0].strip()\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {answer}\\n\")\n",
    "    print(\"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Push to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, create_repo, login\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "repo_id = f\"{HF_USERNAME}/{MODEL_NAME}\"\n",
    "create_repo(repo_id, token=HF_TOKEN, private=False, exist_ok=True)\n",
    "\n",
    "print(f\"Pushing to {repo_id}...\")\n",
    "merged_model.push_to_hub(repo_id, token=HF_TOKEN, use_auth_token=True)\n",
    "tokenizer.push_to_hub(repo_id, token=HF_TOKEN, use_auth_token=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Model live at: https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Create Model Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = f\"\"\"---\n",
    "license: apache-2.0\n",
    "language: [en]\n",
    "tags: [philosophy, wisdom, stoicism, bushido, native-american-wisdom]\n",
    "base_model: {BASE_MODEL}\n",
    "---\n",
    "\n",
    "# üßô The Elder - Wisdom Guide LLM\n",
    "\n",
    "A philosophical AI guide trained on Bushido, Stoicism, and Native American wisdom.\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{repo_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{repo_id}\")\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "response = generator(\"What is true strength?\", max_new_tokens=150)\n",
    "print(response[0]['generated_text'])\n",
    "```\n",
    "\n",
    "## Mobile (GGUF)\n",
    "\n",
    "Download `The_Elder.gguf` for Android LLM apps (SmolChat, LM Studio, etc.)\n",
    "\n",
    "## Training\n",
    "\n",
    "- Base: TinyLlama-1.1B-Chat\n",
    "- Method: LoRA (r=16, Œ±=32)\n",
    "- Dataset: 50+ wisdom Q&A pairs\n",
    "- Epochs: 3\n",
    "\n",
    "---\n",
    "\n",
    "*\"The warrior trains not to avoid falling, but to rise each time with greater wisdom.\"*\n",
    "\"\"\"\n",
    "\n",
    "with open(\"./the-elder-merged/README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"./the-elder-merged/README.md\",\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=repo_id,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model card uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£4Ô∏è‚É£ Convert to GGUF (~10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing llama.cpp...\")\n",
    "!git clone https://github.com/ggerganov/llama.cpp 2>/dev/null || true\n",
    "!cd llama.cpp && make 2>/dev/null || true\n",
    "!pip install -q gguf\n",
    "\n",
    "print(\"\\nConverting to FP16 GGUF...\")\n",
    "!python llama.cpp/convert.py ./the-elder-merged --outtype f16 --outfile ./the-elder-f16.gguf\n",
    "\n",
    "print(\"\\nQuantizing to Q4_K_M...\")\n",
    "!./llama.cpp/quantize ./the-elder-f16.gguf ./The_Elder.gguf Q4_K_M\n",
    "\n",
    "import os\n",
    "gguf_size = os.path.getsize(\"./The_Elder.gguf\") / (1024 * 1024)\n",
    "print(f\"\\n‚úÖ GGUF created: {gguf_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£5Ô∏è‚É£ Upload GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Uploading GGUF...\")\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"./The_Elder.gguf\",\n",
    "    path_in_repo=\"The_Elder.gguf\",\n",
    "    repo_id=repo_id,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ GGUF uploaded!\")\n",
    "print(f\"\\nüì• Download: https://huggingface.co/{repo_id}/resolve/main/The_Elder.gguf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ COMPLETE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéâ THE ELDER - TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úÖ Model: https://huggingface.co/{repo_id}\")\n",
    "print(f\"‚úÖ GGUF: https://huggingface.co/{repo_id}/resolve/main/The_Elder.gguf\")\n",
    "print(f\"‚úÖ Size: {gguf_size:.2f} MB\")\n",
    "print(\"\\nüì± Install on Android:\")\n",
    "print(\"  1. Download The_Elder.gguf\")\n",
    "print(\"  2. Install SmolChat or LM Studio\")\n",
    "print(\"  3. Load GGUF file\")\n",
    "print(\"  4. Chat with The Elder!\")\n",
    "print(\"\\n‚ú® May The Elder guide you on your path ‚ú®\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
